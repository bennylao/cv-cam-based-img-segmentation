{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6460a86",
   "metadata": {},
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48708f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e024487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_dir(cwd: pathlib.Path = pathlib.Path().resolve(), anchor=\"README.md\") -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Get the root directory of the project by searching for a specific anchor file. \n",
    "    i.e. find the root directory where anchor file README.md/.git is located.\n",
    "    \n",
    "    Args:\n",
    "        cwd (pathlib.Path): Current working directory.\n",
    "        anchor (str): The name of the anchor file to search for.\n",
    "\n",
    "    Returns:\n",
    "        pathlib.Path: The root directory of the project.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the anchor file is not found in any parent directories.\n",
    "    \"\"\"\n",
    "    # Check if the anchor file exists in the current working directory\n",
    "    # If it does, return the current working directory\n",
    "    # If it doesn't, check the parent directories until the anchor file is found\n",
    "    if cwd.joinpath(anchor).exists():\n",
    "        return cwd\n",
    "    else:\n",
    "        for parent in cwd.parents:\n",
    "            if (parent / anchor).exists():\n",
    "                return parent\n",
    "    \n",
    "    # If the anchor file is not found in any parent directories, raise an error\n",
    "    raise FileNotFoundError(f\"Anchor file '{anchor}' not found in any parent directories of {cwd}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e50a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Runtime: Local Machine\n",
      "CURRENT_PATH=PosixPath('/home/benny/vscode-projects/cv-cam-based-img-segmentation/notebooks')\n",
      "ROOT=PosixPath('/home/benny/vscode-projects/cv-cam-based-img-segmentation')\n",
      "DATA_DIR=PosixPath('/home/benny/vscode-projects/cv-cam-based-img-segmentation/data')\n",
      "MODEL_DIR=PosixPath('/home/benny/vscode-projects/cv-cam-based-img-segmentation/models')\n",
      "OUTPUT_DIR=PosixPath('/home/benny/vscode-projects/cv-cam-based-img-segmentation/output')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Git repository information\n",
    "REPO_GIT_OWNER = \"bennylao\"\n",
    "REPO_NAME = \"cv-cam-based-img-segmentation\"\n",
    "\n",
    "\n",
    "### Logics to set up paths based on the environment (Google Colab or local machine) ###\n",
    "COLAB_ROOT_PATH = pathlib.Path(\"/content\")\n",
    "IS_COLAB = COLAB_ROOT_PATH.exists()\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Working on Google Colab\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Drive\n",
    "    DRIVE_PATH = COLAB_ROOT_PATH.joinpath(\"drive\")\n",
    "    drive.flush_and_unmount()\n",
    "    drive.mount(str(DRIVE_PATH))\n",
    "\n",
    "    # Load git credentials from Google Drive\n",
    "    DRIVE_FOLDER_PATH = DRIVE_PATH.joinpath(\"MyDrive\", \"Colab Notebooks\")\n",
    "    if DRIVE_FOLDER_PATH.exists():\n",
    "        with open(DRIVE_FOLDER_PATH.joinpath(\"git_credentials.json\"), \"r\") as f:\n",
    "            git_config = json.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Config file not found at {DRIVE_FOLDER_PATH}\")\n",
    "\n",
    "    # Set up Git credentials\n",
    "    GIT_USER_NAME = git_config[\"GIT_USER_NAME\"]\n",
    "    GIT_TOKEN = git_config[\"GIT_TOKEN\"]\n",
    "    GIT_USER_EMAIL = git_config[\"GIT_USER_EMAIL\"]\n",
    "\n",
    "    !git config --global user.email {GIT_USER_EMAIL}\n",
    "    !git config --global user.name {GIT_USER_NAME}\n",
    "\n",
    "    # Set up project paths\n",
    "    CURRENT_PATH = pathlib.Path().resolve()\n",
    "    ROOT = COLAB_ROOT_PATH.joinpath(REPO_NAME)\n",
    "    DATA_DIR = ROOT.joinpath(\"data\")\n",
    "    MODEL_DIR = DRIVE_FOLDER_PATH.joinpath(\"models\")\n",
    "    OUTPUT_DIR = DRIVE_FOLDER_PATH.joinpath(\"output\")\n",
    "\n",
    "    # Clone repo\n",
    "    GIT_PATH = f\"https://{GIT_TOKEN}@github.com/{REPO_GIT_OWNER}/{REPO_NAME}.git\"\n",
    "\n",
    "    if not ROOT.exists():\n",
    "        !git clone --depth 1 \"{GIT_PATH}\" \"{ROOT}\"\n",
    "    else:\n",
    "        print(f\"Git repo already cloned at {ROOT}\")\n",
    "        !git -C \"{ROOT}\" pull\n",
    "\n",
    "else:\n",
    "    # Working on local machine\n",
    "    CURRENT_PATH = pathlib.Path().resolve()\n",
    "    ROOT = get_root_dir(CURRENT_PATH, anchor=\"README.md\")\n",
    "    DATA_DIR = ROOT.joinpath(\"data\")\n",
    "    MODEL_DIR = ROOT.joinpath(\"models\")\n",
    "    OUTPUT_DIR = ROOT.joinpath(\"output\")\n",
    "\n",
    "# Create folder if not exist\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created data directory at {DATA_DIR}\")\n",
    "\n",
    "if not OUTPUT_DIR.exists():\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created output directory at {OUTPUT_DIR}\")\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created model directory at {MODEL_DIR}\")\n",
    "\n",
    "# Add root path to sys.path\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Runtime: {'Google Colab' if IS_COLAB else 'Local Machine'}\")\n",
    "print(f\"{CURRENT_PATH=}\")\n",
    "print(f\"{ROOT=}\")\n",
    "print(f\"{DATA_DIR=}\")\n",
    "print(f\"{MODEL_DIR=}\")\n",
    "print(f\"{OUTPUT_DIR=}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45476cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from src import utils\n",
    "from src.cam.gradcam import generate_multiscale_cam, threshold_cam_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefab856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMAGE_SIZE = 256\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "TRAIN_EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e09e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pin_memory = True if torch.cuda.is_available() else False\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# load dataset\n",
    "train_transforms = utils.Compose([\n",
    "    utils.PILToTensor(),\n",
    "    utils.ResizeImgAndMask(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    utils.ConvertMaskToBinary(),\n",
    "    utils.RandomHorizontalFlip(flip_prob=0.5),\n",
    "    utils.RandomVerticalFlip(flip_prob=0.5),\n",
    "    utils.RandomRotation(degrees=30),\n",
    "    utils.ToDtype(dtype=torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "test_transforms = utils.Compose([\n",
    "    utils.PILToTensor(),\n",
    "    utils.ResizeImgAndMask(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    utils.ConvertMaskToBinary(),\n",
    "    utils.ToDtype(dtype=torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "trainset, testset = utils.construct_dataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec02b5f",
   "metadata": {},
   "source": [
    "## Resnet18 Label Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffe6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 37\n",
    "model = resnet18(weights=\"DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f01774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:20<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 537.8190 | Train Acc: 0.5877 | Test Acc: 0.8118\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:21<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 336.8641 | Train Acc: 0.6787 | Test Acc: 0.7229\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:17<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 229.1401 | Train Acc: 0.7710 | Test Acc: 0.7850\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:20<00:00, 15.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 188.1818 | Train Acc: 0.8093 | Test Acc: 0.7746\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:20<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 173.6308 | Train Acc: 0.8239 | Test Acc: 0.7714\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:17<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 153.7452 | Train Acc: 0.8455 | Test Acc: 0.8463\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:17<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 125.3838 | Train Acc: 0.8762 | Test Acc: 0.8476\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:20<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 144.2324 | Train Acc: 0.8509 | Test Acc: 0.8027\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:20<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 115.9217 | Train Acc: 0.8820 | Test Acc: 0.8018\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:17<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 107.4724 | Train Acc: 0.8871 | Test Acc: 0.8313\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{TRAIN_EPOCHS}\")\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, _, _, labels in tqdm(trainloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    train_acc = correct / len(trainloader.dataset)\n",
    "\n",
    "    test_acc = 0.0\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    test_acc = val_correct / len(testloader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{TRAIN_EPOCHS} | Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc53d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = MODEL_DIR.joinpath(\"resnet18_classifier.pth\")\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b65cd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=\"DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_transforms = utils.Compose([\n",
    "\tutils.PILToTensor(),\n",
    "\tutils.ResizeImgAndMask(size=(256, 256)),\n",
    "\tutils.ToDtype(dtype=torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "test_transforms = utils.Compose([\n",
    "\tutils.PILToTensor(),\n",
    "\tutils.ResizeImgAndMask(size=(256, 256)),\n",
    "\tutils.ToDtype(dtype=torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "with open(DATA_DIR / \"train_ids.json\") as f:\n",
    "    train_ids = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / \"test_ids.json\") as f:\n",
    "    test_ids = json.load(f)\n",
    "\n",
    "cam_output_dir = DATA_DIR.joinpath(\"cam_dataset\")\n",
    "cam_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainset, testset = utils.construct_dataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    ")\n",
    "\n",
    "target_layer = model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cd03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam_images(dataset, ids, save_dir):\n",
    "    for i in tqdm(range(len(dataset)), desc=f\"Generating CAM\"):\n",
    "        sample, _, _, _ = dataset[i]\n",
    "        input_tensor = sample.unsqueeze(0).to(device)\n",
    "\n",
    "        cam = generate_multiscale_cam(\n",
    "            model=model,\n",
    "            image_tensor=input_tensor,\n",
    "            target_layer=target_layer,\n",
    "            target_size= (256, 256),\n",
    "            scales=[128, 256, 512, 1024]\n",
    "        )\n",
    "        # image id\n",
    "        image_id = ids[i]\n",
    "        \n",
    "        # cam \n",
    "        cam = np.clip(cam, 0, 1)\n",
    "        cam_uint8 = (cam * 255).astype(np.uint8)\n",
    "        Image.fromarray(cam_uint8, mode='L').save(save_dir / f\"{image_id}_graycam.png\")\n",
    "        # mask\n",
    "        mask = threshold_cam_three(cam, high_threshold=0.6, low_threshold=0.4)\n",
    "        Image.fromarray(mask.astype(np.uint8), mode='L').save(save_dir / f\"{image_id}_mask.png\")\n",
    "\n",
    "    print(f\"Saved CAM & CAM Mask PNGs to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbaed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating CAM: 100%|██████████| 5144/5144 [15:38<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CAM & CAM Mask PNGs to /home/benny/vscode-projects/cv-cam-based-img-segmentation/data/cam_dataset/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating CAM: 100%|██████████| 2205/2205 [06:14<00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CAM & CAM Mask PNGs to /home/benny/vscode-projects/cv-cam-based-img-segmentation/data/cam_dataset/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_cam_images(trainset, train_ids, cam_output_dir)\n",
    "generate_cam_images(testset, test_ids, cam_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
